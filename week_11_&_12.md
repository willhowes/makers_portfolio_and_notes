# Weeks 11 & 12

## Goals for the final project
* Can you use high-quality processes to build an extended project in a team?

### Week 11 - Final Project kick off
#### Goals

##### Have fun

##### Learn
* New tech
* Teamwork
* Project Management
* Estimations
* Scope

##### Build something you are proud of

##### Have a project that you talk about to employers


#### To consider when planning
* We represent our own client
* Write our own user stories
* Empathise with users 
* Imagine the interaction
* What are you goals - focus on: product, learning, or tech? Or what are the priorities.

#### SCHEDULE
* Only 8 days
* Recommended to have 4 x 2-day sprints

##### WEEK 1
Mon - Agree on: Goals / Team process / MVP / Tech stack / Tools / Slack Channel (Alice, Mike, Kat)
Tue -
Wed - MVP by Wed evening
Thu -
Fri -
##### WEEK 2
Mon - 
Tue - 
Wed - Feature freeze ** NO MORE FEATURES AFTER THIS POINT ** 
Thu - Practice presentation
Fri - No more work on project - maybe practice presentation

## Careers postcourse support workshop
- Remote standups at 10am - Tues, Weds and Thurs
- #Careers and #Mentoring channels
- About your slack channel: https://github.com/makersacademy/jobhunters/blob/master/pills/careersteam/slack.md
- Plenty of commits after Makers
- Careers Fair: 2.30pm Weds. Get there a bit early. Bring a note pad. 
- The Hub application are for the careers team not the employers
- What favourite you pick might be v important
- Projects. Something original. 
- Popular languages: Javascript, React, Python. But look at what employers you want to work are using. 
- Set clear goals
- Don't learn lots of different languages 
- Make something you can actually present
- Something you can walkthrough
- Go to meetups
- Coding should be no 1 priority

## Feedback from Alice on Process (from a couple of weeks ago)
### Alice's notes from the video
Setup  - ok

3:00 - You decided to create a class here (`echo = Echo.new`). This is a common pattern I see with Makers. Why are you creating a class here? Do you have a model in mind? Classes are only necessary if you need to keep state of things.

This early in writing the tests, you should focus on expressing the behaviour.

The behaviour here has to do with user input output. You can either write all of your tests in that vein (see next message) OR you can separate interface (Input/Output to and from STDIN/OUT) and logic. In that case, you can first specify the model and TDD that, and then create a "controller" that manages input output. The overall feature tests are the same (you'll need to stub input and test fro output), but you can start by testing the underlying logic only.

Here, in your first test, you are testing the return value of the response function. (even though your test says "Says..." and not "Returns...")
This is ok if it's clear that you are only building the underlying logic here and will build the interface later on top of it, but at this point there is no sign for me to believe that.

4:00 - Following the errors closely here - cool

5:00 - "First test passing" is not a good commit message. a commit message should tell the person who is reviewing your code what the changes have added/ made happen.
The name/description of your test is often a better description when you commit after a test is green (eg: Adds #response - returns "goodbye" with an input of "exit")

BTW, depending on how you define the first feature, this may be a bit early for a commit.

7:00 - The description of this test could be more precise. The description you used could be a good description for a context block ('repeats what the user said'), which would then be divided into a number of smaller, more specific, tests (these are example, you could condense them into 2 or three tests):
eg:
- "return contains 'Hi' when user inputs 'Hi'"
- "return contains 'Hello' when user inputs 'Hello'"
- "return contains todays date"
- "return contains the time"
- "return is formatted as YYYY-MM-DD | hh-mm | You said '<user_input>'!"


I like the order in which you are choosing which test to do next. These lead you nicely to build more and more complex code, but at the same time, the tests are still valid description of the final behaviour you are shooting for in this method (the return/output thing excepted). This is super important, you should not write tests that you know would be deprecated with the next step of the implementation.

The simplest implementation here was actually

if user_input == 'exit' return 'Goodbye'
'Hi'

8:00 - git commit message slightly better, b ut no need to mention that the test is passing.

10:00 - Good research, but too early. Tests should not be concerned with implementation. By this I mean that the tests should probably use a hard coded date string as expectation. Eg: `expect(echo.response('hi')).to include('2019-09-12')`

Testing is all about "fixing" the environment, to be able to expect very specific results (that will mean time freezing). Also, the test you wrote could fail when run around midnight (that actually happens).


Tip: When researching new ruby functions, like `strptime`, using irb is super super useful. 

16:00 - Here you can see how tests can feel like they are pointless, as your test is using exactly the same code as your implementation.
With a fixed string, you don't have to know how you sill do things in order to still have a test for them.

Also, if you use hard coded result string, you can copy paste the acceptance criteria. And that means you are less likely to make a small mistake. (here you are missing the `!` at the end of the line to be printed)

21:00 +

This last test feels very difficult to write. That's because it's not super defined. And also because the "run" method is now expected to be much more than one method. A good modelling session would help here, separating logic and interface, and clarifying the overall behaviours of the app.
```
#### Test Behaviour
You should think about the behaviour of your code first, and then write tests for that. It will give you more flexibility as the test are not enforcing a specific behaviour, which means you can then implement in different ways, and refactor without changing tests.
The most important thing to look for is - what should you be testing. Then if you have a good idea of what, it's easy to google how.
Here's an example breakdown of behaviours for the echo app:
####  Behaviour 1: printing say something:
| Input           | Output                  |
|-----------------|-------------------------|
| running the app | prints "Say something:" |
#### Behaviour 2: repeating the answer with the time or quit:
| Input              | Output                   |
|--------------------|--------------------------|
| inputs "Hello"     | prints "2019-09-12 | 16:34 | You said: 'Hello'!" |
| inputs "Hi there!" | prints "2019-09-12 | 16:34 | You said: 'Hi there!'!" |
| inputs "exit"      | prints "Goodbye" and quits |
It's useful at this point to think about what type of interaction the user will have with the app:
* Is it a REPL app?
* Is it a CLI app?
* Is it a web app?
Each of these call for quite different interactions.
REPL:  the user calls functions, and sees values returned by the function.
CLI: the user will input text on STDOUT, and read text printed on STDOUT, but not see return values.
Web: the user can navigate to pages, fill forms, click on links and buttons, and they will see what is displayed on pages.
Here we are working on a CLI app, which means you will need to find a way to stub input on STDIN, and test outputs on STDOUT
I included a lot of things in here
Let me know if you have questions :slightly_smiling_face:
